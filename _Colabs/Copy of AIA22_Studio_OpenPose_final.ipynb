{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42177,"status":"ok","timestamp":1654511781845,"user":{"displayName":"Pablo Antuña Molina","userId":"13923138663824272340"},"user_tz":-120},"id":"tPDHGDNV4M_e","outputId":"7d1fb127-beb6-4778-c191-6c833d5e1ba4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==1.13.2\n","  Downloading tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7 MB)\n","\u001b[K     |████████████████████████████████| 92.7 MB 34 kB/s \n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Collecting slim\n","  Downloading slim-0.6.2-py36.py37.py38-none-any.whl (176 kB)\n","\u001b[K     |████████████████████████████████| 176 kB 60.7 MB/s \n","\u001b[?25hCollecting slidingwindow\n","  Downloading slidingwindow-0.0.14-py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.37.1)\n","Collecting tensorboard<1.14.0,>=1.13.0\n","  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n","\u001b[K     |████████████████████████████████| 3.2 MB 52.5 MB/s \n","\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n","  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n","\u001b[K     |████████████████████████████████| 367 kB 35.1 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.17.3)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.21.6)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.46.3)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n","Collecting keras-applications>=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 8.4 MB/s \n","\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.5.3)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.0.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (3.1.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.7)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.2.0)\n","Collecting mock>=2.0.0\n","  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n","Collecting python-multipart==0.0.5\n","  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n","Requirement already satisfied: msgpack<2.0,>=0.5.6 in /usr/local/lib/python3.7/dist-packages (from slim) (1.0.3)\n","Collecting uvicorn<0.12.0,>0.11.0\n","  Downloading uvicorn-0.11.8-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n","\u001b[?25hCollecting multidict<5.0,>=4.5\n","  Downloading multidict-4.7.6-cp37-cp37m-manylinux1_x86_64.whl (149 kB)\n","\u001b[K     |████████████████████████████████| 149 kB 69.0 MB/s \n","\u001b[?25hCollecting dataclasses\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Collecting schematics<2.2.0,>=2.1.0\n","  Downloading schematics-2.1.1-py2.py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n","\u001b[?25hRequirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from slim) (7.1.2)\n","Collecting yarl<1.5,>=1.0\n","  Downloading yarl-1.4.2-cp37-cp37m-manylinux1_x86_64.whl (256 kB)\n","\u001b[K     |████████████████████████████████| 256 kB 64.5 MB/s \n","\u001b[?25hCollecting aiofiles<0.6,>=0.4.0\n","  Downloading aiofiles-0.5.0-py3-none-any.whl (11 kB)\n","Collecting websockets==8.*\n","  Downloading websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.5 MB/s \n","\u001b[?25hCollecting h11<0.10,>=0.8\n","  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 2.4 MB/s \n","\u001b[?25hCollecting httptools==0.1.*\n","  Downloading httptools-0.1.2-cp37-cp37m-manylinux1_x86_64.whl (219 kB)\n","\u001b[K     |████████████████████████████████| 219 kB 67.7 MB/s \n","\u001b[?25hCollecting uvloop>=0.14.0\n","  Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 36.6 MB/s \n","\u001b[?25hRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<1.5,>=1.0->slim) (2.10)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from slidingwindow) (5.4.8)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.2) (1.5.2)\n","Building wheels for collected packages: python-multipart\n","  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=7e4f1b3b2c3b343a83ba6cf2fcc1502b35b75700a2ace86e3f1e574a431c4ec5\n","  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n","Successfully built python-multipart\n","Installing collected packages: websockets, uvloop, multidict, mock, httptools, h11, yarl, uvicorn, tensorflow-estimator, tensorboard, schematics, python-multipart, keras-applications, dataclasses, aiofiles, tensorflow, slim, slidingwindow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.2 which is incompatible.\u001b[0m\n","Successfully installed aiofiles-0.5.0 dataclasses-0.6 h11-0.9.0 httptools-0.1.2 keras-applications-1.0.8 mock-4.0.3 multidict-4.7.6 python-multipart-0.0.5 schematics-2.1.1 slidingwindow-0.0.14 slim-0.6.2 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0 uvicorn-0.11.8 uvloop-0.16.0 websockets-8.1 yarl-1.4.2\n"]}],"source":["!pip install tensorflow==1.13.2 opencv-python slim slidingwindow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8375,"status":"ok","timestamp":1654511793830,"user":{"displayName":"Pablo Antuña Molina","userId":"13923138663824272340"},"user_tz":-120},"id":"FgPCZdoV7wIo","outputId":"734aba22-d6a6-4e84-abf6-3e00c884cb10"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  swig3.0\n","Suggested packages:\n","  swig-doc swig-examples swig3.0-examples swig3.0-doc\n","The following NEW packages will be installed:\n","  swig swig3.0\n","0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 1,100 kB of archives.\n","After this operation, 5,822 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n","Fetched 1,100 kB in 1s (1,764 kB/s)\n","Selecting previously unselected package swig3.0.\n","(Reading database ... 155632 files and directories currently installed.)\n","Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n","Unpacking swig3.0 (3.0.12-1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n","Unpacking swig (3.0.12-1) ...\n","Setting up swig3.0 (3.0.12-1) ...\n","Setting up swig (3.0.12-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}],"source":["!apt install swig"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7443,"status":"ok","timestamp":1654511801267,"user":{"displayName":"Pablo Antuña Molina","userId":"13923138663824272340"},"user_tz":-120},"id":"zQS0qoDH49po","outputId":"139f37d6-08a7-4bda-dff1-ed7ee8d6dee4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'human-action-classification'...\n","remote: Enumerating objects: 339, done.\u001b[K\n","remote: Counting objects: 100% (4/4), done.\u001b[K\n","remote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 339 (delta 0), reused 0 (delta 0), pack-reused 335\u001b[K\n","Receiving objects: 100% (339/339), 102.03 MiB | 31.05 MiB/s, done.\n","Resolving deltas: 100% (91/91), done.\n"]}],"source":["!git clone https://github.com/dronefreak/human-action-classification.git && mv human-action-classification/* . && rm -drf human-action-classification\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4448,"status":"ok","timestamp":1654511805710,"user":{"displayName":"Pablo Antuña Molina","userId":"13923138663824272340"},"user_tz":-120},"id":"M5y9iN5j7P68","outputId":"8c50a038-fa7d-47e7-ef57-5079a5dc102b"},"outputs":[{"output_type":"stream","name":"stdout","text":["running build_ext\n","building '_pafprocess' extension\n","swigging pafprocess.i to pafprocess_wrap.cpp\n","swig -python -c++ -o pafprocess_wrap.cpp pafprocess.i\n","creating build/temp.linux-x86_64-3.7\n","x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I. -I/usr/include/python3.7m -c pafprocess.cpp -o build/temp.linux-x86_64-3.7/pafprocess.o\n","x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I. -I/usr/include/python3.7m -c pafprocess_wrap.cpp -o build/temp.linux-x86_64-3.7/pafprocess_wrap.o\n","x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/pafprocess.o build/temp.linux-x86_64-3.7/pafprocess_wrap.o -o /content/tf_pose/pafprocess/_pafprocess.cpython-37m-x86_64-linux-gnu.so\n"]}],"source":["!cd tf_pose/pafprocess/ && swig -python -c++ pafprocess.i && python setup.py build_ext --inplace"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14545,"status":"ok","timestamp":1654511820248,"user":{"displayName":"Pablo Antuña Molina","userId":"13923138663824272340"},"user_tz":-120},"id":"a8RtPp_TcIGg","outputId":"e588cb2a-61ba-4aad-8f41-bed429b16003"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","\n","#mount drive\n","mount_point= '/content/gdrive'\n","drive.mount(mount_point)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xu2P2-iy_xdZ"},"outputs":[],"source":["import argparse\n","import logging\n","import time\n","import os\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","\n","from tf_pose.estimator import TfPoseEstimator\n","from tf_pose.networks import get_graph_path, model_wh\n","import scripts.label_image as label_img\n","import scripts.label_image_scene as label_img_scene\n","\n","if __name__ == '__main__':\n"," \n","    IMAGE = '/content/gdrive/MyDrive/Colab Notebooks/AIA22_STUDIO/220525_SplitScenes/01/scene3.png'\n","\n","    e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n","    image = cv2.imread(IMAGE)\n","\n","\t# count = 0\n","\t\n","    start_time = time.time()\n","    humans = e.inference(image, upsample_size=4.0)\n","    img = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n","\t\n","    # Getting only the skeletal structure (with white background) of the actual image\n","    image = np.zeros(image.shape,dtype=np.uint8)\n","    image.fill(255) \n","    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n","\t\n","    # Classification\n","    pose_class = label_img.classify(image)\n","    scene_class = label_img_scene.classify(IMAGE)\n","    end_time = time.time()\n","    cv2.putText(img,\n","\t\t\t\t\"Predicted Pose: %s\" %(pose_class),\n","\t\t\t\t(10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n","\t\t\t\t(0, 0, 255), 2)\n","    cv2.putText(img,\n","\t\t\t\t\"Predicted Scene: %s\" %(scene_class),\n","\t\t\t\t(10, 30),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n","\t\t\t\t(0, 0, 255), 2)\n","    print('\\n Overall Evaluation time (1-image): {:.3f}s\\n'.format(end_time-start_time))\n","    cv2.imwrite(f'show_{IMAGE}',img)\n","    cv2_imshow(img)"]},{"cell_type":"markdown","source":["#**MODIFIED OPEN-POSE**\n","\n","It runs over the images in a folder and appends pose and activity to lists for further analysis."],"metadata":{"id":"50aV87QSVghg"}},{"cell_type":"code","source":["import argparse\n","import logging\n","import time\n","import os\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","\n","from tf_pose.estimator import TfPoseEstimator\n","from tf_pose.networks import get_graph_path, model_wh\n","import scripts.label_image as label_img\n","import scripts.label_image_scene as label_img_scene"],"metadata":{"id":"P2oypVk3l0Kj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/gdrive/MyDrive/Colab Notebooks/YOLO_testset/OpenPose_testset/'\n","\n","# Just needed in case you'd like to append it to an array\n","img_paths = []\n","\n","# for filename in os.listdir(path):\n","#     if filename.endswith(\"jpg\" or \"png\"): \n","#         img_paths.append(path + filename)\n","\n","# Some names might be weird and not append properly, when this happens the following way works\n","# Only import this way when you know all the files in the folder are images\n","for filename in os.listdir(path):\n","  img_paths.append(path + filename)"],"metadata":{"id":"YVMsNtUSEvL9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_paths"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fcpo0PfxmTNP","executionInfo":{"status":"ok","timestamp":1654513303149,"user_tz":-120,"elapsed":4,"user":{"displayName":"Pablo Antuña Molina","userId":"13923138663824272340"}},"outputId":"44c3e27b-96ab-4793-8698-e9724f8a5ed0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/gdrive/MyDrive/Colab Notebooks/YOLO_testset/OpenPose_testset/Screenshot 2022-05-12 at 16.08.47.png',\n"," '/content/gdrive/MyDrive/Colab Notebooks/YOLO_testset/OpenPose_testset/Screenshot_Trim1.png',\n"," '/content/gdrive/MyDrive/Colab Notebooks/YOLO_testset/OpenPose_testset/Screenshot_Trim2.png',\n"," '/content/gdrive/MyDrive/Colab Notebooks/YOLO_testset/OpenPose_testset/Screenshot_Trim3.png']"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["# img_paths"],"metadata":{"id":"IegxubbnHuh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["poses = []\n","activities = []\n","\n","if __name__ == '__main__':\n","\n","  for p in img_paths:\n","    # OpenPose estimation modified by pablo, if not working return to source code\n","    IMAGE = p\n","\n","    e = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(432, 368))\n","    image = cv2.imread(IMAGE)\n","\n","    humans = e.inference(image, upsample_size=4.0)\n","    img = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n","\n","    # Getting only the skeletal structure (with white background) of the actual image\n","    image = np.zeros(image.shape,dtype=np.uint8)\n","    image.fill(255) \n","    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n","\n","    # Classification\n","    poses.append(label_img.classify(image))\n","    activities.append(label_img_scene.classify(IMAGE))\n","\n","    #Plotting image and prediction, comment section to save time\n","    cv2.putText(img,\n","\t\t\t\t\"Predicted Pose: %s\" %(label_img.classify(image)),\n","\t\t\t\t(10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n","\t\t\t\t(0, 0, 255), 2)\n","    cv2.putText(img,\n","\t\t\t\t\"Predicted Scene: %s\" %(label_img_scene.classify(IMAGE)),\n","\t\t\t\t(10, 30),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n","\t\t\t\t(0, 0, 255), 2)\n","    cv2.imwrite(f'show_{IMAGE}',img)\n","    cv2_imshow(img)"],"metadata":{"id":"nKEve6Z1DmJi","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1654513398016,"user_tz":-120,"elapsed":90188,"user":{"displayName":"Pablo Antuña Molina","userId":"13923138663824272340"}},"outputId":"8b070d00-30fb-4e37-c886-71b943fdae75"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**EVALUATING POSES**"],"metadata":{"id":"LznnC-PbR6oG"}},{"cell_type":"code","source":["poses = [' '.join(item.split(sep=' ')[:-1]) for item in poses]"],"metadata":{"id":"ANPU7ey2Kf4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["poses"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ECZpuJfjnmtJ","executionInfo":{"status":"ok","timestamp":1654513416489,"user_tz":-120,"elapsed":5,"user":{"displayName":"Pablo Antuña Molina","userId":"13923138663824272340"}},"outputId":"a6360ca5-3db7-44bc-fa2d-b50e90ece274"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['upright', 'upright', 'upright', 'upright']"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["poses_dict = {}\n","\n","for pose in poses:\n","  if pose in poses_dict.keys():\n","    poses_dict[pose] += 1\n","  else:\n","    poses_dict[pose] = 1"],"metadata":{"id":"fpcjcQfKSFvi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["poses_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Y2vaM65SVAv","executionInfo":{"status":"ok","timestamp":1654513448129,"user_tz":-120,"elapsed":403,"user":{"displayName":"Pablo Antuña Molina","userId":"13923138663824272340"}},"outputId":"23610bc5-647f-4e8e-ed0e-3b5068b581f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'upright': 4}"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["num_poses = 3\n","\n","if len(poses_dict) >= num_poses:\n","  max_poses = sorted(poses_dict, key=poses_dict.get, reverse=True)[:num_poses]\n","else:\n","  max_poses = sorted(poses_dict, key=poses_dict.get, reverse=True)"],"metadata":{"id":"lNwVN26ZUBti"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**EVALUATING ACTIVITIES**"],"metadata":{"id":"JXUC4yauR_0H"}},{"cell_type":"code","source":["activities = [' '.join(item.split(sep=' ')[:-1]) for item in activities]"],"metadata":{"id":"Bny0y6QIMydz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activities_relevant = ['applauding',\n","                       'climbing',\n","                       'cooking', \n","                       'cutting vegetables', \n","                       'drinking', \n","                       'fixing a bike', \n","                       'fixing a car', \n","                       'gardening', \n","                       'playing guitar', \n","                       'playing violin', \n","                       'reading', \n","                       'riding a bike', \n","                       'running', \n","                       'taking photos', \n","                       'walking the dog']"],"metadata":{"id":"g0a-TWIXOroQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activities_dict = {}\n","\n","for act in activities:\n","  if act in activities_relevant:\n","    if act in activities_dict.keys():\n","      activities_dict[act] += 1\n","    else:\n","      activities_dict[act] = 1"],"metadata":{"id":"I1eaipxhPnH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activities_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0-t6rBWRqhn","executionInfo":{"status":"ok","timestamp":1654513456243,"user_tz":-120,"elapsed":3,"user":{"displayName":"Pablo Antuña Molina","userId":"13923138663824272340"}},"outputId":"b8551742-1c5c-4e5c-c35a-e8bcddcabd1b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'applauding': 1, 'reading': 1, 'riding a bike': 1}"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["num_activities = 3\n","\n","if len(activities_dict) >= num_activities:\n","  max_activities = sorted(activities_dict, key=activities_dict.get, reverse=True)[:num_activities]\n","else:\n","  max_activities = sorted(activities_dict, key=activities_dict.get, reverse=True)"],"metadata":{"id":"iCVBWVn9TM86"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_activities"],"metadata":{"id":"igS5O13gr-wu","executionInfo":{"status":"ok","timestamp":1654513468298,"user_tz":-120,"elapsed":277,"user":{"displayName":"Pablo Antuña Molina","userId":"13923138663824272340"}},"outputId":"c7137b65-b0a5-482e-a9a6-b1dcd05169f5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['reading', 'riding a bike', 'applauding']"]},"metadata":{},"execution_count":64}]}],"metadata":{"accelerator":"GPU","colab":{"name":"AIA22_Studio_OpenPose_final.ipynb","provenance":[{"file_id":"1m8KwVnt7YXDr-9_QJrR8j0uKuX9usndt","timestamp":1654511668347},{"file_id":"1INGvr8HsbBYfd7vRgPpz4lpvbCXa2E_t","timestamp":1652267708075}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}